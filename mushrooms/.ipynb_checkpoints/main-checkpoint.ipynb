{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0     p         x           s         n       t    p               f   \n",
       "1     e         x           s         y       t    a               f   \n",
       "2     e         b           s         w       t    l               f   \n",
       "3     p         x           y         w       t    p               f   \n",
       "4     e         x           s         g       f    n               f   \n",
       "\n",
       "  gill-spacing gill-size gill-color   ...   stalk-surface-below-ring  \\\n",
       "0            c         n          k   ...                          s   \n",
       "1            c         b          k   ...                          s   \n",
       "2            c         b          n   ...                          s   \n",
       "3            c         n          n   ...                          s   \n",
       "4            w         b          k   ...                          s   \n",
       "\n",
       "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring-number ring-type spore-print-color population habitat  \n",
       "0           o         p                 k          s       u  \n",
       "1           o         p                 n          n       g  \n",
       "2           o         p                 n          n       m  \n",
       "3           o         p                 k          s       u  \n",
       "4           o         e                 n          a       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mushrooms.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataArray = np.array(data)\n",
    "total = len(dataArray)\n",
    "N = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = dataArray[:N,1:]\n",
    "y_train = dataArray[:N,0]\n",
    "#print(X_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = dataArray[N:,1:]\n",
    "y_test = dataArray[N:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts = []\n",
    "codes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes.append('bcxfks')\n",
    "codes.append('fgys')\n",
    "codes.append('nbcgrpuewy')\n",
    "codes.append('tf')\n",
    "codes.append('alcyfmnps')\n",
    "codes.append('adfn')\n",
    "codes.append('cwd')\n",
    "codes.append('bn')\n",
    "codes.append('knbhgropuewy')\n",
    "codes.append('et')\n",
    "codes.append('bcuezr?')\n",
    "codes.append('fyks')\n",
    "codes.append('fyks')\n",
    "codes.append('nbcgopewy')\n",
    "codes.append('nbcgopewy')\n",
    "codes.append('pu')\n",
    "codes.append('nowy')\n",
    "codes.append('not')\n",
    "codes.append('ceflnpsz')\n",
    "codes.append('knbhrouwy')\n",
    "codes.append('acnsvy')\n",
    "codes.append('glmpuwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "    temp = {}\n",
    "    for i in range(0,len(code)):\n",
    "        temp[code[i]] = i\n",
    "    dicts.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m  = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,m):\n",
    "    for x in X_train:\n",
    "        x[j] = dicts[j][x[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 0 0 7 2 0 1 0 0 3 3 3 7 7 0 2 1 5 0 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(0,m):\n",
    "    for x in X_test:\n",
    "        x[j] = dicts[j][x[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i] == 'p':\n",
    "        y_train[i] = 0\n",
    "    else:\n",
    "        y_train[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(y_test)):\n",
    "    if y_test[i] == 'p':\n",
    "        y_test[i] = 0\n",
    "    else:\n",
    "        y_test[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(22,2),\n",
    "                     nn.Softmax())\n",
    "\n",
    "dType = torch.FloatTensor\n",
    "model.type(dType)\n",
    "lossFunc = nn.CrossEntropyLoss().type(dType)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainloader = DataLoader(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.FloatTensor(X_train)\n",
    "X_train_var = torch.autograd.Variable(X_train_torch)\n",
    "ans = model(X_train_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_trch = torch.LongTensor(y_train)\n",
    "y_train_var = torch.autograd.Variable(y_train_trch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  loss =  0.6419903635978699\n",
      "epoch  1  loss =  0.8099180459976196\n",
      "epoch  2  loss =  0.6535382866859436\n",
      "epoch  3  loss =  0.5517061352729797\n",
      "epoch  4  loss =  0.5850035548210144\n",
      "epoch  5  loss =  0.5073103308677673\n",
      "epoch  6  loss =  0.4904598593711853\n",
      "epoch  7  loss =  0.49383744597435\n",
      "epoch  8  loss =  0.5026364922523499\n",
      "epoch  9  loss =  0.5032272338867188\n",
      "epoch  10  loss =  0.4929479658603668\n",
      "epoch  11  loss =  0.48511630296707153\n",
      "epoch  12  loss =  0.48295411467552185\n",
      "epoch  13  loss =  0.48316434025764465\n",
      "epoch  14  loss =  0.4836617708206177\n",
      "epoch  15  loss =  0.4837488532066345\n",
      "epoch  16  loss =  0.4832194447517395\n",
      "epoch  17  loss =  0.4819735288619995\n",
      "epoch  18  loss =  0.4798486530780792\n",
      "epoch  19  loss =  0.47648873925209045\n",
      "epoch  20  loss =  0.47136998176574707\n",
      "epoch  21  loss =  0.46420711278915405\n",
      "epoch  22  loss =  0.45784640312194824\n",
      "epoch  23  loss =  0.45950302481651306\n",
      "epoch  24  loss =  0.4593958258628845\n",
      "epoch  25  loss =  0.44728440046310425\n",
      "epoch  26  loss =  0.4402984380722046\n",
      "epoch  27  loss =  0.4397130310535431\n",
      "epoch  28  loss =  0.4356631934642792\n",
      "epoch  29  loss =  0.427705317735672\n",
      "epoch  30  loss =  0.4247486889362335\n",
      "epoch  31  loss =  0.4261326789855957\n",
      "epoch  32  loss =  0.422561377286911\n",
      "epoch  33  loss =  0.4186813533306122\n",
      "epoch  34  loss =  0.4188065826892853\n",
      "epoch  35  loss =  0.4189715087413788\n",
      "epoch  36  loss =  0.4168608486652374\n",
      "epoch  37  loss =  0.4143234193325043\n",
      "epoch  38  loss =  0.41367900371551514\n",
      "epoch  39  loss =  0.41372841596603394\n",
      "epoch  40  loss =  0.4120807647705078\n",
      "epoch  41  loss =  0.4100181758403778\n",
      "epoch  42  loss =  0.409231036901474\n",
      "epoch  43  loss =  0.40886348485946655\n",
      "epoch  44  loss =  0.4077562987804413\n",
      "epoch  45  loss =  0.40620389580726624\n",
      "epoch  46  loss =  0.40527963638305664\n",
      "epoch  47  loss =  0.4050035774707794\n",
      "epoch  48  loss =  0.40429893136024475\n",
      "epoch  49  loss =  0.4031173884868622\n",
      "epoch  50  loss =  0.40233004093170166\n",
      "epoch  51  loss =  0.40198424458503723\n",
      "epoch  52  loss =  0.4014584720134735\n",
      "epoch  53  loss =  0.4006102681159973\n",
      "epoch  54  loss =  0.39989009499549866\n",
      "epoch  55  loss =  0.3995057940483093\n",
      "epoch  56  loss =  0.3990953266620636\n",
      "epoch  57  loss =  0.39843183755874634\n",
      "epoch  58  loss =  0.3978089690208435\n",
      "epoch  59  loss =  0.3974267244338989\n",
      "epoch  60  loss =  0.3970775306224823\n",
      "epoch  61  loss =  0.39658063650131226\n",
      "epoch  62  loss =  0.3960783779621124\n",
      "epoch  63  loss =  0.39573678374290466\n",
      "epoch  64  loss =  0.39544418454170227\n",
      "epoch  65  loss =  0.39504721760749817\n",
      "epoch  66  loss =  0.39463359117507935\n",
      "epoch  67  loss =  0.3943237066268921\n",
      "epoch  68  loss =  0.39404916763305664\n",
      "epoch  69  loss =  0.3937056064605713\n",
      "epoch  70  loss =  0.3933351933956146\n",
      "epoch  71  loss =  0.39302295446395874\n",
      "epoch  72  loss =  0.3927381634712219\n",
      "epoch  73  loss =  0.3924049735069275\n",
      "epoch  74  loss =  0.39204251766204834\n",
      "epoch  75  loss =  0.39170846343040466\n",
      "epoch  76  loss =  0.3913876712322235\n",
      "epoch  77  loss =  0.3910292387008667\n",
      "epoch  78  loss =  0.3906432092189789\n",
      "epoch  79  loss =  0.39027100801467896\n",
      "epoch  80  loss =  0.3898918926715851\n",
      "epoch  81  loss =  0.38946130871772766\n",
      "epoch  82  loss =  0.3889913558959961\n",
      "epoch  83  loss =  0.3885103166103363\n",
      "epoch  84  loss =  0.3879985809326172\n",
      "epoch  85  loss =  0.3874377906322479\n",
      "epoch  86  loss =  0.386868417263031\n",
      "epoch  87  loss =  0.386338472366333\n",
      "epoch  88  loss =  0.3858552873134613\n",
      "epoch  89  loss =  0.3854259252548218\n",
      "epoch  90  loss =  0.385067343711853\n",
      "epoch  91  loss =  0.3847588896751404\n",
      "epoch  92  loss =  0.38447800278663635\n",
      "epoch  93  loss =  0.3842204809188843\n",
      "epoch  94  loss =  0.3839758038520813\n",
      "epoch  95  loss =  0.3837275803089142\n",
      "epoch  96  loss =  0.383475124835968\n",
      "epoch  97  loss =  0.38322484493255615\n",
      "epoch  98  loss =  0.3829672932624817\n",
      "epoch  99  loss =  0.38270556926727295\n",
      "epoch  100  loss =  0.38244563341140747\n",
      "epoch  101  loss =  0.38218533992767334\n",
      "epoch  102  loss =  0.38192233443260193\n",
      "epoch  103  loss =  0.3816626965999603\n",
      "epoch  104  loss =  0.3814052939414978\n",
      "epoch  105  loss =  0.38114842772483826\n",
      "epoch  106  loss =  0.3808920979499817\n",
      "epoch  107  loss =  0.38063883781433105\n",
      "epoch  108  loss =  0.3803868889808655\n",
      "epoch  109  loss =  0.3801376223564148\n",
      "epoch  110  loss =  0.3798905611038208\n",
      "epoch  111  loss =  0.37964528799057007\n",
      "epoch  112  loss =  0.3793999254703522\n",
      "epoch  113  loss =  0.379154771566391\n",
      "epoch  114  loss =  0.37890568375587463\n",
      "epoch  115  loss =  0.3786495327949524\n",
      "epoch  116  loss =  0.3783775866031647\n",
      "epoch  117  loss =  0.3780825436115265\n",
      "epoch  118  loss =  0.37775033712387085\n",
      "epoch  119  loss =  0.37735864520072937\n",
      "epoch  120  loss =  0.3768748641014099\n",
      "epoch  121  loss =  0.37623655796051025\n",
      "epoch  122  loss =  0.37532714009284973\n",
      "epoch  123  loss =  0.37389934062957764\n",
      "epoch  124  loss =  0.3714015483856201\n",
      "epoch  125  loss =  0.3673575818538666\n",
      "epoch  126  loss =  0.3627600073814392\n",
      "epoch  127  loss =  0.35717248916625977\n",
      "epoch  128  loss =  0.35227227210998535\n",
      "epoch  129  loss =  0.35013148188591003\n",
      "epoch  130  loss =  0.35082393884658813\n",
      "epoch  131  loss =  0.3522932827472687\n",
      "epoch  132  loss =  0.3537125289440155\n",
      "epoch  133  loss =  0.3539670407772064\n",
      "epoch  134  loss =  0.3528381586074829\n",
      "epoch  135  loss =  0.35137513279914856\n",
      "epoch  136  loss =  0.3497830331325531\n",
      "epoch  137  loss =  0.3489389717578888\n",
      "epoch  138  loss =  0.3483094274997711\n",
      "epoch  139  loss =  0.3478512167930603\n",
      "epoch  140  loss =  0.34762999415397644\n",
      "epoch  141  loss =  0.347354531288147\n",
      "epoch  142  loss =  0.3470134735107422\n",
      "epoch  143  loss =  0.3467242121696472\n",
      "epoch  144  loss =  0.34640568494796753\n",
      "epoch  145  loss =  0.3460253179073334\n",
      "epoch  146  loss =  0.3456870913505554\n",
      "epoch  147  loss =  0.34535568952560425\n",
      "epoch  148  loss =  0.3449775278568268\n",
      "epoch  149  loss =  0.3446439504623413\n",
      "epoch  150  loss =  0.3443618416786194\n",
      "epoch  151  loss =  0.34406352043151855\n",
      "epoch  152  loss =  0.343845933675766\n",
      "epoch  153  loss =  0.34368234872817993\n",
      "epoch  154  loss =  0.3434954881668091\n",
      "epoch  155  loss =  0.343334823846817\n",
      "epoch  156  loss =  0.3431289792060852\n",
      "epoch  157  loss =  0.3428914546966553\n",
      "epoch  158  loss =  0.3426690101623535\n",
      "epoch  159  loss =  0.34243056178092957\n",
      "epoch  160  loss =  0.3422347903251648\n",
      "epoch  161  loss =  0.34207847714424133\n",
      "epoch  162  loss =  0.3419407308101654\n",
      "epoch  163  loss =  0.3418445587158203\n",
      "epoch  164  loss =  0.341752290725708\n",
      "epoch  165  loss =  0.3416600525379181\n",
      "epoch  166  loss =  0.3415725827217102\n",
      "epoch  167  loss =  0.3414648771286011\n",
      "epoch  168  loss =  0.341352641582489\n",
      "epoch  169  loss =  0.3412380516529083\n",
      "epoch  170  loss =  0.3411157429218292\n",
      "epoch  171  loss =  0.34100407361984253\n",
      "epoch  172  loss =  0.3409000039100647\n",
      "epoch  173  loss =  0.34080472588539124\n",
      "epoch  174  loss =  0.3407263457775116\n",
      "epoch  175  loss =  0.34065482020378113\n",
      "epoch  176  loss =  0.34059175848960876\n",
      "epoch  177  loss =  0.3405342102050781\n",
      "epoch  178  loss =  0.34047290682792664\n",
      "epoch  179  loss =  0.3404109477996826\n",
      "epoch  180  loss =  0.3403451442718506\n",
      "epoch  181  loss =  0.3402741551399231\n",
      "epoch  182  loss =  0.3402034342288971\n",
      "epoch  183  loss =  0.3401312828063965\n",
      "epoch  184  loss =  0.3400610685348511\n",
      "epoch  185  loss =  0.33999645709991455\n",
      "epoch  186  loss =  0.3399348258972168\n",
      "epoch  187  loss =  0.3398782014846802\n",
      "epoch  188  loss =  0.33982449769973755\n",
      "epoch  189  loss =  0.33977261185646057\n",
      "epoch  190  loss =  0.33972305059432983\n",
      "epoch  191  loss =  0.3396725058555603\n",
      "epoch  192  loss =  0.33962175250053406\n",
      "epoch  193  loss =  0.33957090973854065\n",
      "epoch  194  loss =  0.33951911330223083\n",
      "epoch  195  loss =  0.339467853307724\n",
      "epoch  196  loss =  0.33941754698753357\n",
      "epoch  197  loss =  0.339368611574173\n",
      "epoch  198  loss =  0.33932146430015564\n",
      "epoch  199  loss =  0.33927470445632935\n",
      "epoch  200  loss =  0.33922967314720154\n",
      "epoch  201  loss =  0.33918654918670654\n",
      "epoch  202  loss =  0.3391435146331787\n",
      "epoch  203  loss =  0.3391008675098419\n",
      "epoch  204  loss =  0.3390587568283081\n",
      "epoch  205  loss =  0.3390166461467743\n",
      "epoch  206  loss =  0.3389747142791748\n",
      "epoch  207  loss =  0.33893319964408875\n",
      "epoch  208  loss =  0.33889198303222656\n",
      "epoch  209  loss =  0.338851660490036\n",
      "epoch  210  loss =  0.3388115465641022\n",
      "epoch  211  loss =  0.33877190947532654\n",
      "epoch  212  loss =  0.33873358368873596\n",
      "epoch  213  loss =  0.3386945426464081\n",
      "epoch  214  loss =  0.33865687251091003\n",
      "epoch  215  loss =  0.3386191129684448\n",
      "epoch  216  loss =  0.3385821580886841\n",
      "epoch  217  loss =  0.3385453522205353\n",
      "epoch  218  loss =  0.3385087847709656\n",
      "epoch  219  loss =  0.3384725749492645\n",
      "epoch  220  loss =  0.3384372293949127\n",
      "epoch  221  loss =  0.3384019136428833\n",
      "epoch  222  loss =  0.33836647868156433\n",
      "epoch  223  loss =  0.33833134174346924\n",
      "epoch  224  loss =  0.338297039270401\n",
      "epoch  225  loss =  0.3382626473903656\n",
      "epoch  226  loss =  0.3382287621498108\n",
      "epoch  227  loss =  0.33819517493247986\n",
      "epoch  228  loss =  0.3381618559360504\n",
      "epoch  229  loss =  0.33812853693962097\n",
      "epoch  230  loss =  0.3380960524082184\n",
      "epoch  231  loss =  0.3380630314350128\n",
      "epoch  232  loss =  0.3380306661128998\n",
      "epoch  233  loss =  0.33799853920936584\n",
      "epoch  234  loss =  0.3379669487476349\n",
      "epoch  235  loss =  0.33793526887893677\n",
      "epoch  236  loss =  0.3379036784172058\n",
      "epoch  237  loss =  0.3378728926181793\n",
      "epoch  238  loss =  0.33784201741218567\n",
      "epoch  239  loss =  0.33781108260154724\n",
      "epoch  240  loss =  0.3377804756164551\n",
      "epoch  241  loss =  0.33774998784065247\n",
      "epoch  242  loss =  0.337719589471817\n",
      "epoch  243  loss =  0.3376898467540741\n",
      "epoch  244  loss =  0.33765995502471924\n",
      "epoch  245  loss =  0.337630033493042\n",
      "epoch  246  loss =  0.3376009464263916\n",
      "epoch  247  loss =  0.33757176995277405\n",
      "epoch  248  loss =  0.33754226565361023\n",
      "epoch  249  loss =  0.33751338720321655\n",
      "epoch  250  loss =  0.3374840319156647\n",
      "epoch  251  loss =  0.3374551832675934\n",
      "epoch  252  loss =  0.33742639422416687\n",
      "epoch  253  loss =  0.337397962808609\n",
      "epoch  254  loss =  0.3373694121837616\n",
      "epoch  255  loss =  0.3373411297798157\n",
      "epoch  256  loss =  0.33731281757354736\n",
      "epoch  257  loss =  0.33728480339050293\n",
      "epoch  258  loss =  0.33725643157958984\n",
      "epoch  259  loss =  0.3372284770011902\n",
      "epoch  260  loss =  0.33720019459724426\n",
      "epoch  261  loss =  0.33717212080955505\n",
      "epoch  262  loss =  0.3371444344520569\n",
      "epoch  263  loss =  0.33711618185043335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  264  loss =  0.3370877504348755\n",
      "epoch  265  loss =  0.337059885263443\n",
      "epoch  266  loss =  0.33703145384788513\n",
      "epoch  267  loss =  0.3370034098625183\n",
      "epoch  268  loss =  0.3369751572608948\n",
      "epoch  269  loss =  0.3369469940662384\n",
      "epoch  270  loss =  0.336918443441391\n",
      "epoch  271  loss =  0.3368900716304779\n",
      "epoch  272  loss =  0.3368614614009857\n",
      "epoch  273  loss =  0.33683279156684875\n",
      "epoch  274  loss =  0.3368041217327118\n",
      "epoch  275  loss =  0.3367748558521271\n",
      "epoch  276  loss =  0.33674633502960205\n",
      "epoch  277  loss =  0.33671703934669495\n",
      "epoch  278  loss =  0.33668777346611023\n",
      "epoch  279  loss =  0.33665862679481506\n",
      "epoch  280  loss =  0.3366296589374542\n",
      "epoch  281  loss =  0.33659985661506653\n",
      "epoch  282  loss =  0.3365708589553833\n",
      "epoch  283  loss =  0.3365415334701538\n",
      "epoch  284  loss =  0.33651217818260193\n",
      "epoch  285  loss =  0.336482435464859\n",
      "epoch  286  loss =  0.3364531695842743\n",
      "epoch  287  loss =  0.33642399311065674\n",
      "epoch  288  loss =  0.3363954722881317\n",
      "epoch  289  loss =  0.3363666236400604\n",
      "epoch  290  loss =  0.3363381624221802\n",
      "epoch  291  loss =  0.33631059527397156\n",
      "epoch  292  loss =  0.3362826108932495\n",
      "epoch  293  loss =  0.3362548351287842\n",
      "epoch  294  loss =  0.33622732758522034\n",
      "epoch  295  loss =  0.33620086312294006\n",
      "epoch  296  loss =  0.33617445826530457\n",
      "epoch  297  loss =  0.33614853024482727\n",
      "epoch  298  loss =  0.3361230492591858\n",
      "epoch  299  loss =  0.33609744906425476\n",
      "epoch  300  loss =  0.3360725939273834\n",
      "epoch  301  loss =  0.3360479772090912\n",
      "epoch  302  loss =  0.3360241949558258\n",
      "epoch  303  loss =  0.3360002934932709\n",
      "epoch  304  loss =  0.33597666025161743\n",
      "epoch  305  loss =  0.33595308661460876\n",
      "epoch  306  loss =  0.33593037724494934\n",
      "epoch  307  loss =  0.3359079360961914\n",
      "epoch  308  loss =  0.3358857035636902\n",
      "epoch  309  loss =  0.3358636796474457\n",
      "epoch  310  loss =  0.33584165573120117\n",
      "epoch  311  loss =  0.33582016825675964\n",
      "epoch  312  loss =  0.33579930663108826\n",
      "epoch  313  loss =  0.33577823638916016\n",
      "epoch  314  loss =  0.33575743436813354\n",
      "epoch  315  loss =  0.3357369899749756\n",
      "epoch  316  loss =  0.3357170820236206\n",
      "epoch  317  loss =  0.33569762110710144\n",
      "epoch  318  loss =  0.3356776535511017\n",
      "epoch  319  loss =  0.3356584906578064\n",
      "epoch  320  loss =  0.3356389105319977\n",
      "epoch  321  loss =  0.3356197774410248\n",
      "epoch  322  loss =  0.33560124039649963\n",
      "epoch  323  loss =  0.33558282256126404\n",
      "epoch  324  loss =  0.335563987493515\n",
      "epoch  325  loss =  0.33554625511169434\n",
      "epoch  326  loss =  0.33552783727645874\n",
      "epoch  327  loss =  0.3355100750923157\n",
      "epoch  328  loss =  0.33549225330352783\n",
      "epoch  329  loss =  0.3354746401309967\n",
      "epoch  330  loss =  0.33545786142349243\n",
      "epoch  331  loss =  0.33544063568115234\n",
      "epoch  332  loss =  0.33542296290397644\n",
      "epoch  333  loss =  0.3354060649871826\n",
      "epoch  334  loss =  0.33538898825645447\n",
      "epoch  335  loss =  0.33537307381629944\n",
      "epoch  336  loss =  0.3353567123413086\n",
      "epoch  337  loss =  0.3353407382965088\n",
      "epoch  338  loss =  0.33532431721687317\n",
      "epoch  339  loss =  0.335308700799942\n",
      "epoch  340  loss =  0.33529290556907654\n",
      "epoch  341  loss =  0.3352772295475006\n",
      "epoch  342  loss =  0.33526158332824707\n",
      "epoch  343  loss =  0.3352460265159607\n",
      "epoch  344  loss =  0.3352307379245758\n",
      "epoch  345  loss =  0.33521565794944763\n",
      "epoch  346  loss =  0.33520036935806274\n",
      "epoch  347  loss =  0.3351856470108032\n",
      "epoch  348  loss =  0.33517107367515564\n",
      "epoch  349  loss =  0.3351561725139618\n",
      "epoch  350  loss =  0.33514174818992615\n",
      "epoch  351  loss =  0.3351267874240875\n",
      "epoch  352  loss =  0.33511286973953247\n",
      "epoch  353  loss =  0.3350988030433655\n",
      "epoch  354  loss =  0.3350842595100403\n",
      "epoch  355  loss =  0.33507028222084045\n",
      "epoch  356  loss =  0.3350563645362854\n",
      "epoch  357  loss =  0.33504271507263184\n",
      "epoch  358  loss =  0.33502835035324097\n",
      "epoch  359  loss =  0.33501502871513367\n",
      "epoch  360  loss =  0.3350018262863159\n",
      "epoch  361  loss =  0.33498844504356384\n",
      "epoch  362  loss =  0.33497482538223267\n",
      "epoch  363  loss =  0.3349616229534149\n",
      "epoch  364  loss =  0.33494848012924194\n",
      "epoch  365  loss =  0.3349352180957794\n",
      "epoch  366  loss =  0.33492228388786316\n",
      "epoch  367  loss =  0.3349097669124603\n",
      "epoch  368  loss =  0.33489635586738586\n",
      "epoch  369  loss =  0.3348841369152069\n",
      "epoch  370  loss =  0.33487141132354736\n",
      "epoch  371  loss =  0.3348592221736908\n",
      "epoch  372  loss =  0.33484649658203125\n",
      "epoch  373  loss =  0.33483409881591797\n",
      "epoch  374  loss =  0.3348216414451599\n",
      "epoch  375  loss =  0.3348095118999481\n",
      "epoch  376  loss =  0.33479711413383484\n",
      "epoch  377  loss =  0.334785133600235\n",
      "epoch  378  loss =  0.33477306365966797\n",
      "epoch  379  loss =  0.334761381149292\n",
      "epoch  380  loss =  0.334749698638916\n",
      "epoch  381  loss =  0.3347378671169281\n",
      "epoch  382  loss =  0.3347260653972626\n",
      "epoch  383  loss =  0.3347143232822418\n",
      "epoch  384  loss =  0.33470267057418823\n",
      "epoch  385  loss =  0.33469119668006897\n",
      "epoch  386  loss =  0.3346799910068512\n",
      "epoch  387  loss =  0.3346683382987976\n",
      "epoch  388  loss =  0.3346572816371918\n",
      "epoch  389  loss =  0.33464619517326355\n",
      "epoch  390  loss =  0.33463507890701294\n",
      "epoch  391  loss =  0.3346242606639862\n",
      "epoch  392  loss =  0.3346131145954132\n",
      "epoch  393  loss =  0.33460214734077454\n",
      "epoch  394  loss =  0.3345911204814911\n",
      "epoch  395  loss =  0.33458080887794495\n",
      "epoch  396  loss =  0.3345702886581421\n",
      "epoch  397  loss =  0.3345596492290497\n",
      "epoch  398  loss =  0.3345487117767334\n",
      "epoch  399  loss =  0.33453795313835144\n",
      "epoch  400  loss =  0.3345276117324829\n",
      "epoch  401  loss =  0.33451756834983826\n",
      "epoch  402  loss =  0.3345073163509369\n",
      "epoch  403  loss =  0.3344971537590027\n",
      "epoch  404  loss =  0.33448654413223267\n",
      "epoch  405  loss =  0.334476500749588\n",
      "epoch  406  loss =  0.33446618914604187\n",
      "epoch  407  loss =  0.3344564139842987\n",
      "epoch  408  loss =  0.33444663882255554\n",
      "epoch  409  loss =  0.33443620800971985\n",
      "epoch  410  loss =  0.33442631363868713\n",
      "epoch  411  loss =  0.33441635966300964\n",
      "epoch  412  loss =  0.33440637588500977\n",
      "epoch  413  loss =  0.3343968987464905\n",
      "epoch  414  loss =  0.33438727259635925\n",
      "epoch  415  loss =  0.3343774378299713\n",
      "epoch  416  loss =  0.3343679904937744\n",
      "epoch  417  loss =  0.3343583345413208\n",
      "epoch  418  loss =  0.3343490660190582\n",
      "epoch  419  loss =  0.3343393802642822\n",
      "epoch  420  loss =  0.3343304395675659\n",
      "epoch  421  loss =  0.33432096242904663\n",
      "epoch  422  loss =  0.33431169390678406\n",
      "epoch  423  loss =  0.334302693605423\n",
      "epoch  424  loss =  0.3342938721179962\n",
      "epoch  425  loss =  0.3342844843864441\n",
      "epoch  426  loss =  0.33427536487579346\n",
      "epoch  427  loss =  0.3342663645744324\n",
      "epoch  428  loss =  0.33425697684288025\n",
      "epoch  429  loss =  0.3342481553554535\n",
      "epoch  430  loss =  0.33423930406570435\n",
      "epoch  431  loss =  0.3342305123806\n",
      "epoch  432  loss =  0.3342217803001404\n",
      "epoch  433  loss =  0.33421316742897034\n",
      "epoch  434  loss =  0.33420440554618835\n",
      "epoch  435  loss =  0.33419597148895264\n",
      "epoch  436  loss =  0.3341872990131378\n",
      "epoch  437  loss =  0.334178626537323\n",
      "epoch  438  loss =  0.33417031168937683\n",
      "epoch  439  loss =  0.3341616988182068\n",
      "epoch  440  loss =  0.3341537415981293\n",
      "epoch  441  loss =  0.33414509892463684\n",
      "epoch  442  loss =  0.3341371417045593\n",
      "epoch  443  loss =  0.33412832021713257\n",
      "epoch  444  loss =  0.33412015438079834\n",
      "epoch  445  loss =  0.33411189913749695\n",
      "epoch  446  loss =  0.3341037631034851\n",
      "epoch  447  loss =  0.3340957760810852\n",
      "epoch  448  loss =  0.334087997674942\n",
      "epoch  449  loss =  0.33407968282699585\n",
      "epoch  450  loss =  0.3340718150138855\n",
      "epoch  451  loss =  0.33406397700309753\n",
      "epoch  452  loss =  0.33405545353889465\n",
      "epoch  453  loss =  0.3340478837490082\n",
      "epoch  454  loss =  0.33404016494750977\n",
      "epoch  455  loss =  0.3340325355529785\n",
      "epoch  456  loss =  0.334024578332901\n",
      "epoch  457  loss =  0.33401718735694885\n",
      "epoch  458  loss =  0.3340092897415161\n",
      "epoch  459  loss =  0.3340016305446625\n",
      "epoch  460  loss =  0.33399394154548645\n",
      "epoch  461  loss =  0.3339865803718567\n",
      "epoch  462  loss =  0.3339790999889374\n",
      "epoch  463  loss =  0.3339717388153076\n",
      "epoch  464  loss =  0.3339640200138092\n",
      "epoch  465  loss =  0.3339565098285675\n",
      "epoch  466  loss =  0.33394911885261536\n",
      "epoch  467  loss =  0.33394163846969604\n",
      "epoch  468  loss =  0.33393439650535583\n",
      "epoch  469  loss =  0.333927184343338\n",
      "epoch  470  loss =  0.3339200019836426\n",
      "epoch  471  loss =  0.33391281962394714\n",
      "epoch  472  loss =  0.33390554785728455\n",
      "epoch  473  loss =  0.33389827609062195\n",
      "epoch  474  loss =  0.33389121294021606\n",
      "epoch  475  loss =  0.33388426899909973\n",
      "epoch  476  loss =  0.3338775038719177\n",
      "epoch  477  loss =  0.3338701128959656\n",
      "epoch  478  loss =  0.33386316895484924\n",
      "epoch  479  loss =  0.3338559865951538\n",
      "epoch  480  loss =  0.33384954929351807\n",
      "epoch  481  loss =  0.33384236693382263\n",
      "epoch  482  loss =  0.333835631608963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  483  loss =  0.33382871747016907\n",
      "epoch  484  loss =  0.3338218331336975\n",
      "epoch  485  loss =  0.33381524682044983\n",
      "epoch  486  loss =  0.3338088095188141\n",
      "epoch  487  loss =  0.3338020443916321\n",
      "epoch  488  loss =  0.3337951600551605\n",
      "epoch  489  loss =  0.3337887227535248\n",
      "epoch  490  loss =  0.3337821662425995\n",
      "epoch  491  loss =  0.3337758183479309\n",
      "epoch  492  loss =  0.33376914262771606\n",
      "epoch  493  loss =  0.33376291394233704\n",
      "epoch  494  loss =  0.33375638723373413\n",
      "epoch  495  loss =  0.33375003933906555\n",
      "epoch  496  loss =  0.3337435722351074\n",
      "epoch  497  loss =  0.3337371349334717\n",
      "epoch  498  loss =  0.3337310552597046\n",
      "epoch  499  loss =  0.33372440934181213\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    forward = model(X_train_var)\n",
    "    loss = lossFunc(forward,y_train_var)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch \",epoch, \" loss = \", loss.data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = model(X_train_var)\n",
    "_, pred_train = torch.max(out_train.data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "\n",
      " 0\n",
      " 1\n",
      " 1\n",
      "⋮ \n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.LongTensor of size 7000]\n",
      "\n",
      "6873\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "#print(pred_train.size(0))\n",
    "#print(pred_train)\n",
    "correct = (pred_train == y_train_trch).sum()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_torch = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "X_test_var = torch.autograd.Variable(X_test_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = model(X_test_var)\n",
    "_, pred_test = torch.max(out_test.data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083 0.9635231316725978\n"
     ]
    }
   ],
   "source": [
    "correct = (pred_test == y_test_torch).sum()\n",
    "print(correct, correct/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Linear (22 -> 2)\n",
      "  (1): Softmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
